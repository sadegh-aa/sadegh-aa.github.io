<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script> -->

  <title>Sadegh Aliakbarian</title>
  <meta name="author" content="Sadegh Aliakbarian">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Sadegh Aliakbarian</name>
              </p>
              <p>
                I'm a senior research scientist at <a href="https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge/">Microsoft Mixed Reality & AI Lab (Cambridge, UK)</a>, working on digital humans and generative modeling of human motion.
                I did my PhD at the <a href="https://www.anu.edu.au/">Australian National University</a>, working on generative modeling of human motion and human activity understanding.
                </p>
              <p style="text-align:center">
                <a href="mailto:s.aliakbarian@gmail.com"><i class="fa fa-envelope-square fa-3x"></i></a> &nbsp &nbsp
                <a href="https://www.linkedin.com/in/sadegh-a/"><i class="fa fa-linkedin-square fa-3x"></i></a> &nbsp &nbsp
                <a href="https://scholar.google.com/citations?user=1qXJQ7cAAAAJ&hl=en"><i class="ai ai-google-scholar-square ai-3x"></i></a> &nbsp &nbsp
                <a href="https://twitter.com/aa_sadegh"><i class="fa fa-twitter-square fa-3x"></i></a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/sadegh_bw_2024.png">
            </td>
          </tr>
        </tbody></table>
  
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Latest News</heading>
              <p>
<!--                 <i class="fa fa-check-circle"></i>
                We shipped <a href="https://learn.microsoft.com/en-us/mesh/overview">Microsoft Mesh</a>. Huge congratulations to the team!
                <br>
                <i class="fa fa-check-circle"></i>
                We shipped <a href="https://techcommunity.microsoft.com/t5/microsoft-teams-blog/introducing-mesh-avatars-for-microsoft-teams-in-private-preview/ba-p/3646444">audio-driven Mesh Avatars</a>! Huge congratulations to the team!
                <br> -->
                <i class="fa fa-check-circle"></i>
                Based on the <a href="https://deepgenerativemodels.github.io/">Deep Generative Models</a> lecture by Stefano Ermon, I wrote a <a href="projects/aliakbarian_energy_models_note_2025.pdf">brief tutorial note</a> on energy, score, and diffusion-based models.
                <br>
                <i class="fa fa-check-circle"></i>
                From March 2021, I joined <a href="https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge/">Microsoft Mixed Reality and AI Lab-Cambridge</a> (UK) as a research scientist.
                <br>
                <i class="fa fa-check-circle"></i>
                Finished my PhD on <a href="https://openresearch-repository.anu.edu.au/bitstream/1885/213305/1/Ali%20Akbarian_Thesis_2021.pdf">Deep Sequence Learning for Video Anticipation</a> at the Australian National University.
                <br>
              <i class="fa fa-check-circle"></i>
                Selected for <a href="http://cvpr2020.thecvf.com/reviewer-acknowledgements">Outstanding Reviewer Award</a> at CVPR 2020.
              </p>
            </td>
          </tr>
        </tbody></table>
  
  
        
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <p>
                I'm interested in computer vision and machine learning, with the focus on human understanding, human mesh recovery, generative AI, and 3D vision.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr><td>2025</td></tr>

           <!-- A publication item -->
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <i class="fa fa-newspaper-o"></i>
                <a href="https://microsoft.github.io/DAViD/">
                    <papertitle>DAViD: Data-efficient and Accurate Vision Models from Synthetic Data</papertitle>
                  </a></b>
                  <br>
                  Fatemeh Saleh, <b>Sadegh Aliakbarian</b>, Charlie Hewitt, Lohit Petikam, Xiao-Xian, Antonio Criminisi, Thomas J. Cashman, Tadas Baltru≈°aitis
                  <br>
                <em>International Conference on Computer Vision (ICCV)</em>, 2025
                  <br>
                  [<a href="https://microsoft.github.io/DAViD/">project page</a> /
                  <a href='https://arxiv.org/abs/2507.15365'>arXiv</a> /
                  <a href="https://youtu.be/FQtHC5e1dDg">Video</a> /
                  <a href="https://github.com/microsoft/DAViD">code</a>]
                </td>
              </tr>
    
          <!-- A publication item -->
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <i class="fa fa-newspaper-o"></i>
                <a href="https://balamuruganthambiraja.github.io/3DiFACE/">
                    <papertitle>3DiFACE: Diffusion-based Speech-driven 3D Facial Animation and Editing</papertitle>
                  </a></b>
                  <br>
                  Balamurugan Thambiraja, <b>Sadegh Aliakbarian</b>, Darren Cosker, Justus Thies
                  <br>
                <em>International Conference on 3D Vision (3DV)</em>, 2025
                  <br>
                  [<a href="https://balamuruganthambiraja.github.io/3DiFACE/">project page</a> /
                  <a href='https://arxiv.org/abs/2312.00870'>arXiv</a> /
                  <a href="https://www.youtube.com/watch?v=Mep5pAU3TPc">Video</a> /
                  <a href="https://github.com/bala1144/3DiFACE">code</a>]
                </td>
              </tr>
    
          <tr><td>2024</td></tr>
          <!-- A publication item -->
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <i class="fa fa-newspaper-o"></i>
                <a href="https://microsoft.github.io/SynthMoCap/">
                    <papertitle>Look Ma, no markers: holistic performance capture without the hassle</papertitle>
                  </a></b>
                  <br>
                Charlie Hewitt, Fatemeh Saleh, <b>Sadegh Aliakbarian</b>, Lohit Petikam, Shideh Rezaeifar, Louis Florentin, Zafiirah Hosenie, Thomas J Cashman, Julien Valentin, Darren Cosker, Tadas Baltrusaitis
                  <br>
                <em>ACM Transactions on Graphics (TOG) / SIGGRAPH Asia</em>, 2024
                  <br>
                  [<a href="https://microsoft.github.io/SynthMoCap/">project page</a> /
                  <a href='https://arxiv.org/pdf/2410.11520'>arXiv</a> /
                  <a href="https://youtu.be/4RkLDW3GmdY">Video</a> /
                  <a href="https://github.com/microsoft/SynthMoCap">code</a>]
                </td>
              </tr>
    
          <!-- A publication item -->
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <i class="fa fa-newspaper-o"></i>
                <a href="https://arxiv.org/abs/2401.14785">
                    <papertitle>SimpleEgo: Predicting probabilistic body pose from egocentric cameras</papertitle>
                  </a></b>
                  <br>
                Hanz Cuevas Velasquez, Charlie Hewitt, <b>Sadegh Aliakbarian</b>, Tadas Baltrusaitis
                  <br>
                <em>International Conference on 3D Vision (3DV)</em>, 2024
                  <br>
                  [<a href="https://microsoft.github.io/SimpleEgo/">project page</a> /
                  <a href='https://arxiv.org/abs/2401.14785'>arXiv</a> /
                  Video /
                  <a href="https://github.com/microsoft/SimpleEgo">code</a>]
                </td>
              </tr>

          <tr><td>2023</td></tr>

          <!-- A publication item -->
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <i class="fa fa-newspaper-o"></i>
                <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Aliakbarian_HMD-NeMo_Online_3D_Avatar_Motion_Generation_From_Sparse_Observations_ICCV_2023_paper.html">
                    <papertitle>HMD-NeMo: Online 3D Avatar Motion Generation From Sparse Observations</papertitle>
                  </a></b>
                  <br>
                <b>Sadegh Aliakbarian</b>, Fatemeh Saleh, David Collier, Pashmina Cameron, Darren Cosker
                  <br>
                <em>International Conference on Computer Vision (ICCV)</em>, 2023
                  <br>
                  [project page /
                  arXiv /
                  Video /
                  code]
                </td>
              </tr>

          
          
              <!-- A publication item -->
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <i class="fa fa-newspaper-o"></i>
                <a href="https://arxiv.org/abs/2301.00023">
                    <papertitle>Imitator: Personalized Speech-driven 3D Facial Animation</papertitle>
                  </a></b>
                  <br>
                  Balamurugan Thambiraja, Ikhsanul Habibie, <b>Sadegh Aliakbarian</b>, Darren Cosker, Christian Theobalt, Justus Thies
                  <br>
                <em>International Conference on Computer Vision (ICCV)</em>, 2023 
                  <br>
                  [<a href="https://balamuruganthambiraja.github.io/Imitator">project page</a> /
                  <a href="https://arxiv.org/abs/2301.00023">arXiv</a> /
                  Video /
                  code]
                </td>
              </tr>

              <!-- A publication item -->
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <i class="fa fa-newspaper-o"></i>
                <a href="https://arxiv.org/abs/2304.06024">
                    <papertitle>Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views</papertitle>
                  </a></b>
                  <br>
                  Siwei Zhang, Qianli Ma, Yan Zhang, <b>Sadegh Aliakbarian</b>, Darren Cosker, Siyu Tang
                  <br>
                <em>International Conference on Computer Vision (ICCV)</em>, 2023 <b>(Oral Presentation)</b>
                  <br>
                  [<a href="https://sanweiliti.github.io/egohmr/egohmr.html">project page</a> /
                  <a href="https://arxiv.org/abs/2304.06024">arXiv</a> /
                  <a href="https://youtu.be/Fh5vtB2pbq0">Video</a> /
                  <a href="https://github.com/sanweiliti/EgoHMR">code</a>]
                </td>
              </tr>

          <tr><td>2022</td></tr>

<!-- A publication item -->
<tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
      <i class="fa fa-newspaper-o"></i>
      <a href="https://arxiv.org/abs/2203.05789">
          <papertitle>FLAG: Flow-based Avatar Generation from Sparse Observations</papertitle>
        </a></b>
        <br>
      <b>Sadegh Aliakbarian</b>, Pashmina Cameron, Federica Bogo, Andew Fitzgibbon, Thomas J. Cashman
        <br>
      <em>International Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022
        <br>
        [<a href="https://microsoft.github.io/flag/"> project page</a> /
      <a href="https://arxiv.org/abs/2203.05789">arXiv</a> /
        <a href="https://www.youtube.com/watch?v=ZgkJ_TD_CRI">Talk</a> /
        code]
      </td>
    </tr>

    <tr><td>2021</td></tr>
    <!-- A publication item -->
  <tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
      <i class="fa fa-newspaper-o"></i>
      <a href="https://arxiv.org/abs/1912.08521v4">
          <papertitle>Contextually Plausible and Diverse 3D Human Motion Prediction</papertitle>
        </a></b>
        <br>
      <b>Sadegh Aliakbarian</b>, Fatemeh Saleh, Lars Petersson, Stephen Gould, Mathieu Salzmann
        <br>
      <em>International Conference on Computer Vision (ICCV)</em>, 2021 <b>(Oral Presentation)</b>
        <br>
        [project page /
        <a href="https://arxiv.org/pdf/1912.08521v4.pdf">arXiv</a> /
        video /
        code]
      </td>
    </tr>
<!-- A publication item -->
  <tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
      <i class="fa fa-newspaper-o"></i>
      <a href="https://arxiv.org/pdf/2012.02337">
          <papertitle>Probabilistic Tracklet Scoring and Inpainting for Multiple Object Tracking</papertitle>
        </a></b>
        <br>
      Fatemeh Saleh, <b>Sadegh Aliakbarian</b>, Hamid Rezatofighi, Mathieu Salzmann, Stephen Gould
        <br>
      <em>Proceedings of the IEEE international conference on computer vision (CVPR)</em>, 2021 <b>(Oral Presentation)</b>
        <br>
        [project page /
        <a href="https://arxiv.org/pdf/2012.02337">arXiv</a> /
        video /
      <a href="https://github.com/fatemeh-slh/ArTIST">code</a>]
      </td>
    </tr>
      
 
<!-- A publication item -->

            <tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
      <i class="fa fa-newspaper-o"></i>
      <a href="https://link.springer.com/article/10.1007/s00138-021-01174-w">
          <papertitle>Multi-FAN: Multi-Spectral Mosaic Super-Resolution Via Multi-Scale Feature Aggregation Network</papertitle>
        </a></b>
        <br>
      Mehrdad Shoeiby, <b>Sadegh Aliakbarian</b>, Saeed Anwar and Lars petersson
      <br>
      <em>Machine Vision and Applications (MVA)</em>, 2021
        <br>
        [project page /
        <a href="https://arxiv.org/pdf/1807.03528.pdf">arXiv</a> /
        video /
      code]
      </td>
    </tr>


<!-- A publication item -->
      
            <tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
      <i class="fa fa-newspaper-o"></i>
      <a href="https://arxiv.org/abs/2009.03075">
          <papertitle>Uncertainty Inspired RGB-D Saliency Detection</papertitle>
        </a></b>
        <br>
      Jing Zhang, Deng-Ping Fan, Yuchao Dai, Saeed Anwar, Fatemeh Saleh, <b>Sadegh Aliakbarian</b>, Nick Barnes
        <br>
      <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2021
        <br>
        [project page /
        <a href="https://arxiv.org/pdf/2009.03075.pdf">arXiv</a> /
        video /
        <a href="https://github.com/JingZhang617/UCNet">code</a>]
      </td>
    </tr>

<tr><td>2020</td></tr>
<!-- A publication item -->
  <tr>
<!--         <td style="padding:20px;width:15%;vertical-align:middle">
     <img src='images/mix_match.png' width="100" height="100">
  </td> -->
  <td style="padding:20px;width:100%;vertical-align:middle">
      <i class="fa fa-newspaper-o"></i>
      <a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Aliakbarian_A_Stochastic_Conditioning_Scheme_for_Diverse_Human_Motion_Prediction_CVPR_2020_paper.html">
          <papertitle>A Stochastic Conditioning Scheme for Diverse Human Motion Prediction</papertitle>
        </a></b>
        <br>
      <b>Sadegh Aliakbarian</b>, Fatemeh Saleh, Mathieu Salzmann, Lars Petersson, Stephen Gould
        <br>
  <em>Proceedings of the IEEE international conference on computer vision (CVPR)</em>, 2020
        <br>
        [<a href="https://mix-and-match.github.io/">project page</a> /
        <a href="https://mix-and-match.github.io/media/MixMatch.pdf">arXiv</a> /
        <a href="https://www.youtube.com/watch?v=2kmihoa8xLA">video</a> /
        <a href="https://github.com/mix-and-match/mix-and-match-tutorial">code</a>]

      </td>
    </tr>
<!-- A publication item -->
  <tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
      <i class="fa fa-newspaper-o"></i>
      <a href="http://openaccess.thecvf.com/content_WACV_2020/papers/Shoeiby_Super-resolved_Chromatic_Mapping_of_Snapshot_Mosaic_Image_Sensors_via_a_WACV_2020_paper.pdf">
          <papertitle>Super-resolved Chromatic Mapping of Snapshot Mosaic Image Sensors via a Texture Sensitive Residual Network</papertitle>
        </a></b>
        <br>
      Mehrdad Shoeiby, Lars Petersson, Ali Armin, <b>Sadegh Aliakbarian</b>
        <br>
  <em>Winter Conference on Applications of Computer Vision (WACV)</em>, 2020
        <br>
        [project page /
        <a href="http://openaccess.thecvf.com/content_WACV_2020/papers/Shoeiby_Super-resolved_Chromatic_Mapping_of_Snapshot_Mosaic_Image_Sensors_via_a_WACV_2020_paper.pdf">arXiv</a> /
        Video /
        Code]

      </td>
    </tr>
<!-- A publication item -->
  <tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
      <i class="fa fa-newspaper-o"></i>
      <a href="https://www.researchgate.net/profile/Saeed_Anwar16/publication/338238878_Mosaic_Super-resolution_via_Sequential_Feature_Pyramid_Networks/links/5e1d157a299bf10bc3abe5f5/Mosaic-Super-resolution-via-Sequential-Feature-Pyramid-Networks.pdf">
          <papertitle>Mosaic Super-resolution via Sequential Feature Pyramid Networks</papertitle>
        </a></b>
        <br>
      Mehrdad Shoeiby, Ali Armin, <b>Sadegh Aliakbarian</b>, Saeed Anwar, Lars Petersson
        <br>
  <em>Proceedings of the IEEE international conference on computer vision Workshops (CVPRW)</em>, 2020
        <br>
        [project page /
        <a href="https://www.researchgate.net/profile/Saeed_Anwar16/publication/338238878_Mosaic_Super-resolution_via_Sequential_Feature_Pyramid_Networks/links/5e1d157a299bf10bc3abe5f5/Mosaic-Super-resolution-via-Sequential-Feature-Pyramid-Networks.pdf">arXiv</a> /
        Video /
        Code]
      </td>
    </tr>

    <tr><td>2019</td></tr>
<!-- A publication item -->
  <tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
      <i class="fa fa-newspaper-o"></i>
      <a href="https://arxiv.org/pdf/1912.08521.pdf">
          <papertitle>Sampling Good Latent Variables via CPP-VAEs: VAEs with Condition Posterior as Prior</papertitle>
        </a></b>
        <br>
      <b>Sadegh Aliakbarian</b>, Fatemeh Saleh, Mathieu Salzmann, Lars Petersson, Stephen Gould
        <br>
  <em>ArXiv</em>, 2019
        <br>
        [project page /
        <a href="https://arxiv.org/pdf/1912.08521.pdf">arXiv</a> /
        Video /
        Code]
      </td>
    </tr>
    <tr><td>2018</td></tr>
<!-- A publication item -->
  <tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
      <i class="fa fa-newspaper-o"></i>
      <a href="https://link.springer.com/chapter/10.1007/978-3-030-20887-5_28">
          <papertitle>VIENA2: A Driving Anticipation Dataset</papertitle>
        </a></b>
        <br>
      <b>Sadegh Aliakbarian</b>, Fatemeh Saleh, Mathieu Salzmann, Basura Fernando, Lars Petersson, Lars Andersson
        <br>
  <em>Asian Conference on Computer Vision (ACCV)</em>, 2018
        <br>
      [<a href="https://sites.google.com/view/viena2-project/home">project page</a> /
        <a href="https://arxiv.org/pdf/1810.09044.pdf">arXiv</a> /
        Video /
      <a href="https://sites.google.com/view/viena2-project/home">Code</a>]
      </td>
    </tr>
<!-- A publication item -->
  <tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
      <i class="fa fa-newspaper-o"></i>
      <a href="https://link.springer.com/chapter/10.1007/978-3-030-01216-8_6">
          <papertitle>Effective Use of Synthetic Data in Urban Scene Semantic Segmentation</papertitle>
        </a></b>
        <br>
      Fatemeh Saleh, <b>Sadegh Aliakbarian</b>, Mathieu Salzmann, Lars Petersson, Jose M Alvarez
        <br>
  <em>European Conference on Computer Vision (ECCV)</em>, 2018
        <br>
      [<a href="https://github.com/fatemeh-slh/VEIS">project page</a> /
        <a href="https://arxiv.org/pdf/1807.06132.pdf">arXiv</a> /
        Video /
      <a href="https://github.com/fatemeh-slh/VEIS">Code</a>]
      </td>
    </tr>

<!-- A publication item -->
  <tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
      <i class="fa fa-newspaper-o"></i>
      <a href="https://ieeexplore.ieee.org/abstract/document/7944591/">
          <papertitle>Incorporating Network Built-in Priors in Weakly-supervised Semantic Segmentation</papertitle>
        </a></b>
        <br>
      Fatemeh Saleh, <b>Sadegh Aliakbarian</b>, Mathieu Salzmann, Lars Petersson, Jose M Alvarez, Stephen Gould
        <br>
  <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2018
        <br>
      [project page /
        <a href="https://arxiv.org/pdf/1706.02189.pdf">arXiv</a> /
        Video /
      Code]
      </td>
    </tr>
<!-- A publication item -->
<tr><td>2017</td></tr>
  <tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
      <i class="fa fa-newspaper-o"></i>
      <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Aliakbarian_Encouraging_LSTMs_to_ICCV_2017_paper.pdf">
          <papertitle>Encouraging LSTMs to Anticipate Actions Very Early</papertitle>
        </a></b>
        <br>
      <b>Sadegh Aliakbarian</b>, Fatemeh Saleh, Mathieu Salzmann, Basura Fernando, Lars Petersson, Lars Andersson
        <br>
  <em>International Conference on Computer Vision (ICCV)</em>, 2017
        <br>
      [project page /
        <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Aliakbarian_Encouraging_LSTMs_to_ICCV_2017_paper.pdf">arXiv</a> /
        Video /
      Code]
      </td>
    </tr>
<!-- A publication item -->
  <tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
      <i class="fa fa-newspaper-o"></i>
      <a href="https://ieeexplore.ieee.org/abstract/document/8237494">
          <papertitle>Bringing background into the foreground: Making all classes equal in weakly-supervised video semantic segmentation</papertitle>
        </a></b>
        <br>
      Fatemeh Saleh, <b>Sadegh Aliakbarian</b>, Mathieu Salzmann, Lars Petersson, Jose M Alvarez
        <br>
  <em>International Conference on Computer Vision (ICCV)</em>, 2017
        <br>
      [project page /
        <a href="https://arxiv.org/pdf/1708.04400.pdf">arXiv</a> /
        Video /
      Code]
      </td>
    </tr>
    <tr><td>2016</td></tr>
<!-- A publication item -->
  <tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
      <i class="fa fa-newspaper-o"></i>
      <a href="https://link.springer.com/chapter/10.1007/978-3-319-46484-8_25">
          <papertitle>Built-in Foreground/Background Prior for Weakly-Supervised Semantic Segmentation</papertitle>
        </a></b>
        <br>
      Fatemeh Saleh, <b>Sadegh Aliakbarian</b>, Mathieu Salzmann, Lars Petersson, Stephen Gould, Jose M Alvarez
        <br>
  <em>European Conference on Computer Vision (ECCV)</em>, 2016
        <br>
      [project page /
        <a href="https://arxiv.org/pdf/1609.00446.pdf">arXiv</a> /
        Video /
      Code]
      </td>
    </tr>

<!-- A publication item -->
        </tbody></table>

          </div>
      </div>
    </section><!-- End About Section -->

    <p style="text-align:center">
      <a href="mailto:s.aliakbarian@gmail.com"><i class="fa fa-envelope-square fa-3x"></i></a> &nbsp &nbsp
      <a href="https://www.linkedin.com/in/sadegh-a/"><i class="fa fa-linkedin-square fa-3x"></i></a> &nbsp &nbsp
      <a href="https://scholar.google.com/citations?user=1qXJQ7cAAAAJ&hl=en"><i class="ai ai-google-scholar-square ai-3x"></i></a> &nbsp &nbsp
      <a href="https://twitter.com/aa_sadegh"><i class="fa fa-twitter-square fa-3x"></i></a>
    </p>
<p align="center">&nbsp;</p>

<p align="right"><font size="2"><a href="http://jonbarron.info">Thanks for the template</a></font></p><br />
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
</body>

</html>
