<!--{% extends "bootstrap/base.html" %}-->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/js/bootstrap.min.js"></script>

  <head>
    <meta http-equiv="CACHE-CONTROL" content="NO-CACHE">
<meta name="description" content="Sadegh Aliakbarian is PhD student at the Australian National University and Australian Centre for Robotic Vision (ACRV). I'm honored to work under supervision of Dr. Lars Petersson (ANU/CSIRO), Dr. Mathieu Salzmann (EPFL), and Dr. Basura Fernando (ACRV/A-STAR) and have pieces of advice from Dr. Stephen Gould (ANU/ACRV).
His areas of interests are in Machine Learning and AI, with focus on generative models and deep learning approaches for computer vision problems.">
<meta name="keywords" content="Australia, Canberra, Australian Centre for Robotic Vision, Data61, ANU, CSIRO, ACRV, Machine Learning, Computer Vision, Generative Models, The Australian National University, Action Recognition, Action Anticipation, VAE, Variational Autoencoders, conditional VAE, CVAE, Posterior Collapse, Semantic Segmentation, Weakly-supervised, Video analysis, pose, pose estimation, Human Motion Prediction">
<meta name="author" content="Sadegh Aliakbarian">
    <title>Sadegh's homepage</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">  
  <script language="JavaScript">
        function ShowHide(divId)
        {
            if(document.getElementById(divId).style.display == 'none')
            {
                document.getElementById(divId).style.display='block';
            }
            else
            {
                document.getElementById(divId).style.display = 'none';
            }
        }
		
		function showaboutMeDisplay() {
				if (document.getElementById("divabout").style.display == "none")
				{
					document.getElementById("divabout").style.display = "block";
				}
				else
				{
					document.getElementById("divabout").style.display = "none";
				}
		}
		
		function showAbstract(divId)
		{
			if(document.getElementById(divId).style.display == 'none')
            {
                document.getElementById(divId).style.display='block';
            }
            else
            {
                document.getElementById(divId).style.display = 'none';
            }
		}
		
        </script>

<style type="text/css">
	
.fa {
  padding: 20px;
  font-size: 30px;
  width: 30px;
  text-align: center;
  text-decoration: none;
  margin: 5px 2px;
  border-radius: 50%;
}

.fa:hover {
    opacity: 0.7;
}

.fa-twitter {
  background: #55ACEE;
  color: white;
}

.fa-google {
  background: #dd4b39;
  color: white;
}

.fa-linkedin {
  background: #007bb5;
  color: white;
}

.fa-instagram {
  background: #125688;
  color: white;
}

.fa-skype {
  background: #00aff0;
  color: white;
}
body {
	font-size:10pt;
	margin:30px;
	background-color:#FFFFFF;
	}
	
	span.titleSt {
	font-size:10pt;
	color: #299cf0;
	font-weight:bold;
	}
	
	span.author {
	font-size:10pt;
	color: #000000;
	}
	
	span.conf {
		font-size:10pt;
		color: #111;
	}
	span.PDF {
		font-size:10pt;
		color: #555;
	}
	span.Note {
		font-size:10pt;
		color: #900;
	}
	
	
	div.ex
{
width:100%;
padding:10px;
border:0px solid gray;
margin:0px;
}

div.bib{
        margin:15px;
		border:2px solid #a1a1a1;
		padding:5px 10px; 
		background:#dddddd;
		width:75%;
		border-radius:15px;
		font-size:75%;
}

div.abstract{
        margin:15px;
		border:2px solid #a1a1a1;
		padding:5px 10px; 
		background:#dddddd;
		width:90%;
		border-radius:15px;
		font-size:75%;
}

img{border:1px solid #;
-webkit-border-radius: 30px;
-moz-border-radius: 30px;
border-radius: 10px;
}

tr.spaceUnder > td
{
  padding-bottom: 2em;
}

tr.spaceUnder2 > td
{
  padding-bottom: 3em;
}

span.year {
		font-size:12pt;
		color: #000000;
		font-weight:bold;
	}

/*Button Style*/ .button { float:left; height:auto; font:96%/150% "Lucida Grande", Geneva, Verdana, Arial, Helvetica, sans-serif; width:12em; text-align:center; white-space:nowrap; } /*Button Arrow Styles*/ .arrows { font-size:90%; margin:0.2em; } /*Button link styles*/ .button a:link, .button a:visited { color: #eee; background-color:#1e5184; font-size:1em; font-weight:bolder; text-decoration: none; border-bottom:0.1em solid #555; border-right:0.1em solid #555; border-top:0.1em solid #ccc; border-left:0.1em solid #ccc; margin: 0.2em; padding:0.2em; display:block; } .button a:hover { background-color:#003366; color:#999; border-top:0.1em solid #777; border-left:0.1em solid #777; border-bottom:0.1em solid #aaa; border-right:0.1em solid #aaa; padding:0.2em; margin: 0.2em; }

a:visited {
    color:#0000FF;
}
	
</style>


  </head>

<div class="container">
<h1>Sadegh Aliakbarian’s homepage</h1>
<span class="glyphicon glyphicon-envelope"></span> <a href="mailto:sadegh.aliakbarian@anu.edu.au">sadegh[dot]aliakbarian[at]anu[dot]edu[dot]au</a> <br/>
<span class="glyphicon glyphicon-map-marker"></span> Level G, Building 801, CSIRO, Black Mountain Lab, Canberra, Australia<br/>
<span class="glyphicon glyphicon-map-marker"></span> College of Engineering and Computer Science, Australian National University, Canberra, Australia<br/>

<br/>

You can also find me on 
&nbsp &nbsp<font size="4"><a href="https://twitter.com/aa_sadegh" class="fa fa-twitter" target="_blank"></a></font>
&nbsp &nbsp<font size="4"><a href="https://www.linkedin.com/in/sadegh-a/" class="fa fa-linkedin" target="_blank"></a>	</font>
&nbsp &nbsp<font size="4"><a href="https://www.instagram.com/sadegh.aliakbarian/?hl=en" class="fa fa-instagram" target="_blank"></a>	</font>
&nbsp &nbsp<font size="4"><a href="https://scholar.google.com/citations?hl=en&user=1qXJQ7cAAAAJ&view_op=list_works&gmla=AJsN-F6sNlU2EVLBgPPBU3TAYIdxX_QWk-WbNpnGl5tVHErQfh8B8bnehZXSEcZKFP74vIgrDaiaoxWpcV8CW6Qywh5ekUHQnE4R4L-UujvuY_VGux1Z1ow" class="fa fa-google" target="_blank"></a>	</font>
<br/>
<hr/>
	

<div class="panel panel-default">
      <div class="panel-heading"><h4>Short bio</h4></div>
      <div class="panel-body">
	<div id="photo">
	<img style="height: 150px;float:left;margin:5px 5px"  src="/images/sadegh.png">
	<div id="divabout" class="ex" >

	I'm a PhD student at the <a href="anu.edu.au" class="text-primary">Australian National University</a> and <a href="https://www.roboticvision.org/rv_person/sadegh-aliakbarian/"  class="text-primary">Australian Centre for Robotic Vision (ACRV)</a>. 
	I'm honored to work under supervision of Dr. Lars Petersson (ANU/CSIRO), Dr. Mathieu Salzmann (EPFL), and Dr. Basura Fernando (ACRV/A-STAR) and have pieces of advice from Dr. Stephen Gould (ANU/ACRV). 
		I also spent some time at <a href="https://www.qualcomm.com/invention/artificial-intelligence/ai-research" class="text-primary">Qualcomm AI Research</a> in Amsterdam as a deep learning research engineering intern, focusing on deep generative models to generate multi-modal, stochastic sequences.
	My areas of interests are in Machine Learning and AI, with focus on generative models and deep learning approaches for computer vision problems. Find out more about my research <a href="https://scholar.google.com/citations?user=1qXJQ7cAAAAJ&hl=en"  class="text-primary">here</a>.
	</div>
</div>
</div>
</div>
<br/>

<div class="panel panel-default">
      <div class="panel-heading"><h4>News</h4></div>
      <div class="panel-body">
	      <ul>
  <li>One paper accepted at ACCV 2018! VIENA2 dataset is now <a href="https://sites.google.com/view/viena2-project/home" class="text-primary">publicly available</a>.</li>
  <li>[April 2018] I'm now a deep learning research/engineering intern at <a href="https://www.qualcomm.com/invention/artificial-intelligence/ai-research" class="text-primary">Qualcomm AI Research</a>, Amsterdam, Netherlands</li>
  <li>[July 2017]  I'm now an associated AI/ML researcher at <a href="https://www.roboticvision.org/rv_person/sadegh-aliakbarian/" class="text-primary">Australian Centre for Robotics Vision (ACRV)</a>, Canberra, Australia</li>
  <li>One paper accepted in ECCV 2018 (with the <a href="https://github.com/fatemehSLH/VEIS">VEIS dataset</a>), one paper in TPAMI 2018, two papers in ICCV 2017, and one paper in ECCV 2016!</li>
</ul>
	</div>
</div>


<br/>

<div class="panel panel-default">
      <div class="panel-heading"><h4>Blog posts</h4></div>
      <div class="panel-body">
  <ul>
  <li><a href='/blogs/vae.html'>An introduction to variational autoencoders (VAEs).</a></li>
</ul>
	</div>
</div>
<br/>	
<!-- <div class="panel panel-default">
      <div class="panel-heading"><h4>Google Scholar</h4></div>
      <div class="panel-body">
  <iframe src="https://scholar.google.com.au/citations?user=1qXJQ7cAAAAJ&hl=en" width="100%" height="300" scrolling="yes">
</iframe>
	</div>
</div>
<br/>	-->
	
<div class="panel panel-default">
      <div class="panel-heading"><h4>Recent experiences</h4></div>
      <div class="panel-body">
  <ul>
  <li><i>Deep Learning Research Engineer Intern</i> | <b>Qualcomm AI Research</b> | Amsterdam, Netherlands (May 2018 - October 2018)</li>
  <li><i>Associate Researcher</i>  | <b>Australian Centre for Robotic Vision (ACRV)</b>, Canberra, Australia (November 2017 - Now)</li>
  <li><i>Research Assistant</i>  | <b>Smart Vision System, Data61, CSIRO</b>, Canberra, Australia (July 2016 - Now)</li>
  <li><i>Machine Learning Research Intern</i> | <b>National ICT Australia (NICTA)</b>, Canberra, Australia (June 2015 - March 2016)</li>
</ul>
	</div>
</div>
<br/>
	
	

<div class="panel panel-default">
      <div class="panel-heading"><h4>Eductation</h4></div>
      <div class="panel-body">
  <ul>
  <li><i>PhD in Computer Science</i> | <b>The Australian National University</b>, Canberra, Australia (July 2016 - Now)</li>
  <li><i>B.Sc. in Computer Software Engineering</i> | <b>Isfahan University of Technology</b>, Isfahan, Iran (October 2009 - September 2013)</li>
</ul>
	</div>
</div>
	
<br/>

<div class="panel panel-default">
      <div class="panel-heading"><h4>Recent publications</h4></div>
      <div class="panel-body">
<table>

<!-- <tr class="spaceUnder2"><td><h5> 2018</h5><td/><td><td/></tr> -->
	
<!-- A publication item -->
  <tr class="spaceUnder">
	<td width="180px" valign="top"><img width="160px" src="/images/VIENA2.png"></td>
	<td valign="top">
	<div class="ex">
	<span class="titleSt">VIENA2: A Driving Anticipation Dataset</span></br>
	<span class="author">Mohammad Sadegh Aliakbarian, Fatemehsadat Saleh, Mathieu Salzmann, Basura Fernando, Lars Petersson, Lars Andersson</span></br>
	<span class="conf">Asian Conference on Computer Vision (ACCV 2018)</span></br>
	<span class="Note">The VIENA2 dataset is available <a href="https://sites.google.com/view/viena2-project/home" class="text-primary">here</a>.</span></br>
	<span class="PDF">
	<a  href="https://arxiv.org/pdf/1810.09044" target="_blank" style="font-size:10pt;color: #555;"> PDF</a> 
	<a onclick="javascript:ShowHide(&#39;AliakbarianACCV18&#39;)" href="javascript:;" style="font-size:10pt;color: #555;" >Bibtex</a>
                    <div class="bib" id="AliakbarianACCV18" style="display: none;">
			@article{aliakbarian2018viena2,</br>
			  title={VIENA2: A Driving Anticipation Dataset},</br>
			  author={Aliakbarian, Mohammad Sadegh and Saleh, Fatemeh Sadat and Salzmann, Mathieu and Fernando, Basura and Petersson, Lars and Andersson, Lars},</br>
			  journal={Asian Conference on Computer Vision (ACCV 2018)},</br>
			  year={2018}</br>
			}</br>
			<button class="close" onclick="document.getElementById('AliakbarianACCV18').style.display='none'" ><font color=red>&times;</font></button>							
                    </div>	
	<a onclick="javascript:ShowHide(&#39;AliakbarianACCV18abs&#39;)" href="javascript:;" style="font-size:10pt;color: #555;" >Abstract</a>
                    <div class="abstract" id="AliakbarianACCV18abs" style="display: none;">
			    Action anticipation is critical in scenarios where one needs to react before the action is finalized. This is, for instance, the case in automated driving, where a car needs to, eg, avoid hitting pedestrians and respect traffic lights. While solutions have been proposed to tackle subsets of the driving anticipation tasks, by making use of diverse, task-specific sensors, there is no single dataset or framework that addresses them all in a consistent manner. In this paper, we therefore introduce a new, large-scale dataset, called VIENA2, covering 5 generic driving scenarios, with a total of 25 distinct action classes. It contains more than 15K full HD, 5s long videos acquired in various driving conditions, weathers, daytimes and environments, complemented with a common and realistic set of sensor measurements. This amounts to more than 2.25 M frames, each annotated with an action label, corresponding to 600 samples per action class. We discuss our data acquisition strategy and the statistics of our dataset, and benchmark state-of-the-art action anticipation techniques, including a new multi-modal LSTM architecture with an effective loss function for action anticipation in driving scenarios. </br>
			<button class="close" onclick="document.getElementById('AliakbarianACCV18abs').style.display='none'" ><font color=red>&times;</font></button>							
                    </div>	

	</span></br>	
	</div>
</td>
</tr>

<!-- A publication item -->
  <tr class="spaceUnder">
	<td width="180px" valign="top"><img width="160px" src="/images/VEIS.png"></td>
	<td valign="top">
	<div class="ex">
	<span class="titleSt">Effective Use of Synthetic Data in Urban Scene Semantic Segmentation</span></br>
	<span class="author">Fatemeh Sadat Saleh, Mohammad Sadegh Aliakbarian, Mathieu Salzmann, Lars Petersson, Jose M. Alvarez</span></br>
	<span class="conf">European Conference on Computer Vision (ECCV 2018)</span></br>
	<span class="Note">The VEIS dataset is available <a href="https://github.com/fatemehSLH/VEIS" class="text-primary">here</a>.</span></br>
	<span class="PDF">
	<a  href="https://arxiv.org/pdf/1807.06132.pdf" target="_blank" style="font-size:10pt;color: #555;"> PDF</a> 
	<a onclick="javascript:ShowHide(&#39;SalehECCV18&#39;)" href="javascript:;" style="font-size:10pt;color: #555;" >Bibtex</a>
                    <div class="bib" id="SalehECCV18" style="display: none;">
			@inproceedings{saleh2018effective,</br>
			  title={Effective use of synthetic data for urban scene semantic segmentation},</br>
			  author={Saleh, Fatemeh Sadat and Aliakbarian, Mohammad Sadegh and Salzmann, Mathieu and Petersson, Lars and Alvarez, Jose M},</br>
			  booktitle={European Conference on Computer Vision},</br>
			  pages={86--103},</br>
			  year={2018},</br>
			  organization={Springer}</br>
			}</br>
			<button class="close" onclick="document.getElementById('SalehECCV18').style.display='none'" ><font color=red>&times;</font></button>							
                    </div>	
<a onclick="javascript:ShowHide(&#39;SalehECCV18abs&#39;)" href="javascript:;" style="font-size:10pt;color: #555;" >Abstract</a>
                    <div class="abstract" id="SalehECCV18abs" style="display: none;">
			    Training a deep network to perform semantic segmentation requires large amounts of labeled data. To alleviate the manual effort of annotating real images, researchers have investigated the use of synthetic data, which can be labeled automatically. Unfortunately, a network trained on synthetic data performs relatively poorly on real images. While this can be addressed by domain adaptation, existing methods all require having access to real images during training. In this paper, we introduce a drastically different way to handle synthetic images that does not require seeing any real images at training time. Our approach builds on the observation that foreground and background classes are not affected in the same manner by the domain shift, and thus should be treated differently. In particular, the former should be handled in a detection-based manner to better account for the fact that, while their texture in synthetic …
                        </br>
			<button class="close" onclick="document.getElementById('SalehECCV18abs').style.display='none'" ><font color=red>&times;</font></button>							
                    </div>
	</span></br>	
	</div>
</td>
</tr>


<!-- A publication item -->
  <tr class="spaceUnder">
	<td width="180px" valign="top"><img width="160px" src="/images/PAMI18.png"></td>
	<td valign="top">
	<div class="ex">
	<span class="titleSt">Incorporating Network Built-in Priors in Weakly-supervised Semantic Segmentation</span></br>
	<span class="author">Fatemeh Saleh, Sadegh Aliakbarian, Mathieu Salzmann, Lars Petersson, Jose M Alvarez, Stephen Gould</span></br>
	<span class="conf">IEEE transactions on pattern analysis and machine intelligence (TPAMI 2018)</span></br>
	<span class="PDF">
	<a  href="https://arxiv.org/pdf/1706.02189" target="_blank" style="font-size:10pt;color: #555;"> PDF</a> 
	<a onclick="javascript:ShowHide(&#39;SalehPAMI18&#39;)" href="javascript:;" style="font-size:10pt;color: #555;" >Bibtex</a>
                    <div class="bib" id="SalehPAMI18" style="display: none;">
			@article{saleh2017incorporating,</br>
			  title={Incorporating network built-in priors in weakly-supervised semantic segmentation},</br>
			  author={Saleh, Fatemeh Sadat and Aliakbarian, Mohammad Sadegh and Salzmann, Mathieu and Petersson, Lars and Alvarez, Jose M and Gould, Stephen},</br>
			  journal={IEEE transactions on pattern analysis and machine intelligence},</br>
			  volume={40},</br>
			  number={6},</br>
			  pages={1382--1396},</br>
			  year={2017},</br>
			  publisher={IEEE}</br>
			}</br>
			<button class="close" onclick="document.getElementById('SalehPAMI18').style.display='none'" ><font color=red>&times;</font></button>							
                    </div>
<a onclick="javascript:ShowHide(&#39;SalehPAMI18abs&#39;)" href="javascript:;" style="font-size:10pt;color: #555;" >Abstract</a>
                    <div class="abstract" id="SalehPAMI18abs" style="display: none;">
Pixel-level annotations are expensive and time consuming to obtain. Hence, weak supervision using only image tags could have a significant impact in semantic segmentation. Recently, CNN-based methods have proposed to fine-tune pre-trained networks using image tags. Without additional information, this leads to poor localization accuracy. This problem, however, was alleviated by making use of objectness priors to generate foreground/background masks. Unfortunately these priors either require pixel-level annotations/bounding boxes, or still yield inaccurate object boundaries. Here, we propose a novel method to extract accurate masks from networks pre-trained for the task of object recognition, thus forgoing external objectness modules. We first show how foreground/ background masks can be obtained from the activations of higher-level convolutional layers of a network. We then show how to obtain multi …
                        </br>
			<button class="close" onclick="document.getElementById('SalehPAMI18abs').style.display='none'" ><font color=red>&times;</font></button>							
                    </div>
	</span></br>	
	</div>
</td>
</tr>


<!-- A publication item -->
  <tr class="spaceUnder">
	<td width="180px" valign="top"><img width="160px" src="/images/EncouragingLSTMs.png"></td>
	<td valign="top">
	<div class="ex">
	<span class="titleSt">Encouraging LSTMs to Anticipate Actions Very Early</span></br>
	<span class="author">Mohammad Sadegh Aliakbarian, Fatemehsadat Saleh, Mathieu Salzmann, Basura Fernando, Lars Petersson and Lars Andersson</span></br>
	<span class="conf">International Conference on Computer Vision (ICCV 2017)</span></br>
	<span class="Note">State-of-the-art in action anticipation!</span></br>
	<span class="PDF">
	<a  href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Aliakbarian_Encouraging_LSTMs_to_ICCV_2017_paper.pdf" target="_blank" style="font-size:10pt;color: #555;"> PDF</a> 
	<a onclick="javascript:ShowHide(&#39;AliakbarianICCV17&#39;)" href="javascript:;" style="font-size:10pt;color: #555;" >Bibtex</a>
                    <div class="bib" id="AliakbarianICCV17" style="display: none;">
			@inproceedings{sadegh2017encouraging,</br>
			  title={Encouraging lstms to anticipate actions very early},</br>
			  author={Sadegh Aliakbarian, Mohammad and Sadat Saleh, Fatemeh and Salzmann, Mathieu and Fernando, Basura and Petersson, Lars and Andersson, Lars},</br>
			  booktitle={Proceedings of the IEEE International Conference on Computer Vision},</br>
			  pages={280--289},</br>
			  year={2017}</br>
			}</br>
			<button class="close" onclick="document.getElementById('AliakbarianICCV17').style.display='none'" ><font color=red>&times;</font></button>							
                    </div>
<a onclick="javascript:ShowHide(&#39;AliakbarianICCV17abs&#39;)" href="javascript:;" style="font-size:10pt;color: #555;" >Abstract</a>
                    <div class="abstract" id="AliakbarianICCV17abs" style="display: none;">
In contrast to the widely studied problem of recognizing an action given a complete sequence, action anticipation aims to identify the action from only partially available videos. As such, it is therefore key to the success of computer vision applications requiring to react as early as possible, such as autonomous navigation. In this paper, we propose a new action anticipation method that achieves high prediction accuracy even in the presence of a very small percentage of a video sequence. To this end, we develop a multi-stage LSTM architecture that leverages context-aware and action-aware features, and introduce a novel loss function that encourages the model to predict the correct class as early as possible. Our experiments on standard benchmark datasets evidence the benefits of our approach; We outperform the state-of-the-art action anticipation methods for early prediction by a relative increase in accuracy of 22.0% on JHMDB-21, 14.0% on UT-Interaction and 49.9% on UCF-101.
                        </br>
			<button class="close" onclick="document.getElementById('AliakbarianICCV17abs').style.display='none'" ><font color=red>&times;</font></button>							
                    </div>
	</span></br>	
	</div>
</td>
</tr>


<!-- A publication item -->
  <tr class="spaceUnder">
	<td width="180px" valign="top"><img width="160px" src="/images/BringingBG2FG.png"></td>
	<td valign="top">
	<div class="ex">
	<span class="titleSt">Bringing background into the foreground: Making all classes equal in weakly-supervised video semantic segmentation</span></br>
	<span class="author">Fatemeh Sadat Saleh, Mohammad Sadegh Aliakbarian, Mathieu Salzmann, Lars Petersson, Jose M Alvarez</span></br>
	<span class="conf">International Conference on Computer Vision (ICCV 2017)</span></br>
	<span class="Note">The first weakly-supervised video semantic segmentation approach.</span></br>
	<span class="PDF">
	<a  href="https://arxiv.org/pdf/1708.04400" target="_blank" style="font-size:10pt;color: #555;"> PDF</a> 
	<a onclick="javascript:ShowHide(&#39;SalehICCV17&#39;)" href="javascript:;" style="font-size:10pt;color: #555;" >Bibtex</a>
                    <div class="bib" id="SalehICCV17" style="display: none;">
			@inproceedings{saleh2017bringing,</br>
			  title={Bringing background into the foreground: Making all classes equal in weakly-supervised video semantic segmentation},</br>
			  author={Saleh, Fatemeh Sadat and Aliakbarian, Mohammad Sadegh and Salzmann, Mathieu and Petersson, Lars and Alvarez, Jose M},</br>
			  booktitle={2017 IEEE international conference on computer vision (ICCV)},</br>
			  pages={2125--2135},</br>
			  year={2017},</br>
			  organization={IEEE}</br>
			}</br>
			<button class="close" onclick="document.getElementById('SalehICCV17').style.display='none'" ><font color=red>&times;</font></button>							
                    </div>	
<a onclick="javascript:ShowHide(&#39;SalehICCV17abs&#39;)" href="javascript:;" style="font-size:10pt;color: #555;" >Abstract</a>
                    <div class="abstract" id="SalehICCV17abs" style="display: none;">
Pixel-level annotations are expensive and time-consuming to obtain. Hence, weak supervision using only image tags could have a significant impact in semantic segmentation. Recent years have seen great progress in weakly-supervised semantic segmentation, whether from a single image or from videos. However, most existing methods are designed to handle a single background class. In practical applications, such as autonomous navigation, it is often crucial to reason about multiple background classes. In this paper, we introduce an approach to doing so by making use of classifier heatmaps. We then develop a two-stream deep architecture that jointly leverages appearance and motion, and design a loss based on our heatmaps to train it. Our experiments demonstrate the benefits of our classifier heatmaps and of our two-stream architecture on challenging urban scene datasets and on the YouTube-Objects …
                        </br>
			<button class="close" onclick="document.getElementById('SalehICCV17abs').style.display='none'" ><font color=red>&times;</font></button>							
                    </div>
	</span></br>	
	</div>
</td>
</tr>



<!-- A publication item -->
  <tr class="spaceUnder">
	<td width="180px" valign="top"><img width="160px" src="/images/BuiltInFGBG.png"></td>
	<td valign="top">
	<div class="ex">
	<span class="titleSt">Built-in Foreground/Background Prior for Weakly-Supervised Semantic Segmentation</span></br>
	<span class="author">Fatemehsadat Saleh, Mohammad Sadegh Aliakbarian, Mathieu Salzmann, Lars Petersson, Stephen Gould, Jose M. Alvarez</span></br>
	<span class="conf">European Conference on Computer Vision (ECCV 2016)</span></br>
	<span class="PDF">
	<a  href="https://arxiv.org/pdf/1609.00446.pdf" target="_blank" style="font-size:10pt;color: #555;"> PDF</a> 
	<a onclick="javascript:ShowHide(&#39;SalehECCV16&#39;)" href="javascript:;" style="font-size:10pt;color: #555;" >Bibtex</a>
                    <div class="bib" id="SalehECCV16" style="display: none;">
			@inproceedings{saleh2016built,
			  title={Built-in foreground/background prior for weakly-supervised semantic segmentation},</br>
			  author={Saleh, Fatemehsadat and Aliakbarian, Mohammad Sadegh and Salzmann, Mathieu and Petersson, Lars and Gould, Stephen and Alvarez, Jose M},</br>
			  booktitle={European Conference on Computer Vision},</br>
			  pages={413--432},</br>
			  year={2016},</br>
			  organization={Springer}</br>
			}</br>
			<button class="close" onclick="document.getElementById('SalehECCV16').style.display='none'" ><font color=red>&times;</font></button>							
                    </div>	
<a onclick="javascript:ShowHide(&#39;SalehECCV16abs&#39;)" href="javascript:;" style="font-size:10pt;color: #555;" >Abstract</a>
                    <div class="abstract" id="SalehECCV16abs" style="display: none;">
Pixel-level annotations are expensive and time consuming to obtain. Hence, weak supervision using only image tags could have a significant impact in semantic segmentation. Recently, CNN-based methods have proposed to fine-tune pre-trained networks using image tags. Without additional information, this leads to poor localization accuracy. This problem, however, was alleviated by making use of objectness priors to generate foreground/background masks. Unfortunately these priors either require training pixel-level annotations/bounding boxes, or still yield inaccurate object boundaries. Here, we propose a novel method to extract markedly more accurate masks from the pre-trained network itself, forgoing external objectness modules. This is accomplished using the activations of the higher-level convolutional layers, smoothed by a dense CRF. We demonstrate that our method, based on these masks …
                        </br>
			<button class="close" onclick="document.getElementById('SalehECCV16abs').style.display='none'" ><font color=red>&times;</font></button>							
                    </div>
	</span></br>	
	</div>
</td>
</tr>

</table>
</div>
	</div>

<br/>
<div class="panel panel-default">
      <div class="panel-heading"><h4>Scholarships and Awards</h4></div>
      <div class="panel-body">

  <ul>
  <li>Recipient of €18K grant for R&D from Qualcomm AI Research, 2018</li>
  <li>Recipient of full scholarship award from ANU of $94K, Australia, 2016</li>
  <li>Recipient of travel grant award from ANU of $7K, Australia, 2016</li>
  <li>Recipient of CSIRO Top-up Award of $35K, Australia, 2016</li>
  <li>Recipient of NICTA Project grant of $10K, Australia, 2016</li>
</ul>
</div>
	</div>

</div>

